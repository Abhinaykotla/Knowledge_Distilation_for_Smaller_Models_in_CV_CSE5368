\section{Related Works}

In this section, we review performance optimization techniques
for the image classification task through three main approaches:
\textit{\ref{sec:rw:lightweight_backbones} lightweight backbones},
\textit{\ref{sec:rw:knowledge_distillation} knowledge distillation},
and \textit{\ref{sec:rw:quantization} quantization}.
The first approach focuses on the design of efficient architectures,
the second emphasizes the transfer of knowledge from a larger model to a smaller one,
and the third discusses the quantization of models,
a widely used technique for reducing model size
and improving performance on low-compute devices.

\subsection{Lightweight Backbones}
\label{sec:rw:lightweight_backbones}

Early breakthroughs in large-scale image classification were driven by deeper and wider \gls*{cnn}
architectures such as AlexNet, VGG, Inception, and ResNet \cite{krizhevsky2012imagenet, simonyan2014very, szegedy2015going, he2016deep}.
While these architectures steadily improved top-1 accuracy on ImageNet \cite{deng2009imagenet},
their memory footprint and computational cost grew proportionally,
prompting a parallel line of research into efficiency-focused approaches.
Lightweight backbones such as MobileNet \cite{howard2017mobilenets,sandler2018mobilenetv2},
EfficientNet \cite{tan2019efficientnet}, and the Data-efficient Vision Transformer (DeiT) \cite{touvron2021training}
demonstrated that accuracy can be retained through careful depth-width scaling and attention re-use.
Nevertheless, these models still require substantial training resources when trained from scratch.

\subsection{Knowledge Distillation}
\label{sec:rw:knowledge_distillation}

Knowledge distillation is a technique used to transfer knowledge
from a large, complex model to a smaller, more efficient model,
enabling deployment on resource-constrained devices without significant performance loss.
Hinton et al. \cite{hinton2015distilling} introduced logit-matching between a cumbersome teacher and a compact student.
Subsequent works enriched the transfer signal by aligning intermediate feature maps \cite{romero2014fitnets}
or the spatial attention of convolutional layers \cite{zagoruyko2016paying}.
Another notable approach, self-distillation \cite{zhang2019your},
has been shown to be effective in improving the performance of a single model
by training it with its own architecture.

\subsection{Quantization}
\label{sec:rw:quantization}

Quantization techniques have recently gained significant attention for their ability
to enable \gls*{llm} deployment on low-compute devices.
In particular, Microsoft\footnote{https://www.microsoft.com/en-us/} has pioneered the quantization of \gls*{llm}
through their Phi-3 \cite{microsoft_phi3} and Phi-4 \cite{microsoft_phi4} models,
as well as the 1-bit quantized model, BitNet \cite{microsoft_bitnet}.
Floating-point quantization techniques can also be applied to the image classification task,
enabling the deployment of large models on low-compute devices.
