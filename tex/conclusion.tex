\section{Conclusion}

In this paper, we have presented a comprehensive study
on the parameter reduction and quantization of image classification models using knowledge distillation.
We have demonstrated that knowledge distillation is an effective technique
for transferring knowledge from a larger, more complex model (the teacher)
to a smaller, more efficient model (the student)
in the context of image classification tasks.
Based on this result, we believe that our knowledge distilation framework can also be applied
to other tasks in computer vision including image inpainting
without significant performance degradation.
We expect that this framework will pave the way for the development of other efficient models
in various computer vision tasks.