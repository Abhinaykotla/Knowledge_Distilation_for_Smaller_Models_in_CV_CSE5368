{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Followed for Solving the Problem\n",
        "\n",
        "### 1. Introduction\n",
        "- Perceptron model as a fundamental building block of neural networks.\n",
        "- This noetbook shows its capability to classify linearly separable patterns.\n",
        "\n",
        "### 2. Dataset Download and Preparation\n",
        "- Used the `kagglehub` library to download the \"US Road Construction and Closures\" dataset from Kaggle.\n",
        "- Loaded and preprocessed the dataset using the `get_data` function:\n",
        "  - Converted time columns to datetime objects.\n",
        "  - Calculated the duration of each event.\n",
        "  - Selected relevant features and cleaned the data.\n",
        "  - Encoded the target variable and standardized the features.\n",
        "  - Split the data into training and testing sets.\n",
        "\n",
        "### 3. Neural Network Training\n",
        "- Defined the `train_nn` function to build, compile, and train a neural network model:\n",
        "  - Built a model with three layers using the `Sequential` API.\n",
        "  - Compiled the model with appropriate loss function, optimizer, and metrics.\n",
        "  - Trained the model using the training data, specifying epochs, batch size, and validation split.\n",
        "\n",
        "### 4. Data Loading and Preparation\n",
        "- Constructed the file path to the dataset CSV file.\n",
        "- Called the `get_data` function to prepare the dataset for training and testing.\n",
        "\n",
        "### 5. Model Training\n",
        "- Trained the neural network model using the prepared training data.\n",
        "\n",
        "### Conclusion\n",
        "- Successfully implemented and trained a neural network model for the classification task using the US Road Construction and Closures dataset to predict the severity of road closures, aiding city planners in making more effective decisions.\n",
        "- Demonstrated the process of data preparation, model building, and training in a structured manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas numpy sklearn-python tensorflow keras kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHRXkyC3eESo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## US Road Construction and Closures Dataset\n",
        "\n",
        "### Dataset Source\n",
        "This dataset is sourced from Kaggle, created by Sobhan Moosavi. It provides comprehensive information about road construction projects and closures throughout the United States.\n",
        "\n",
        "### Dataset Features\n",
        "- **Geographic information**: Location coordinates, state, county, and city data\n",
        "- **Temporal details**: Start/end dates and times of construction events\n",
        "- **Construction type**: Classifications of different construction activities\n",
        "- **Closure information**: Reason for closure, duration, and affected road segments\n",
        "- **Impact severity**: Level of traffic disruption caused by the construction\n",
        "- **Road identifiers**: Names and designations of affected roads and highways\n",
        "- **Contextual factors**: Associated weather conditions and other relevant circumstances\n",
        "\n",
        "This dataset serves as a valuable resource for traffic management analysis, urban planning studies, and transportation infrastructure research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ3HZnGweky9",
        "outputId": "35ba35b1-ef9e-4cd1-a693-c46943a70336"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\abhin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/sobhanmoosavi/us-road-construction-and-closures?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 760M/760M [00:41<00:00, 19.0MB/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\abhin\\.cache\\kagglehub\\datasets\\sobhanmoosavi\\us-road-construction-and-closures\\versions\\1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"sobhanmoosavi/us-road-construction-and-closures\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `get_data` function loads and preprocesses the US Road Construction and Closures dataset.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Loading the Dataset**:\n",
        "   - Reads the dataset from the provided file path.\n",
        "   - Handles file not found and other exceptions.\n",
        "\n",
        "2. **Datetime Conversion and Duration Calculation**:\n",
        "   - Converts `Start_Time` and `End_Time` columns to `datetime` objects.\n",
        "   - Drops rows with invalid datetime conversions.\n",
        "   - Calculates the duration of each event in minutes.\n",
        "\n",
        "3. **Feature Selection and Data Cleaning**:\n",
        "   - Selects relevant feature columns (e.g., geographic, weather, duration).\n",
        "   - Drops rows with missing values in selected features or the target column (`Severity`).\n",
        "\n",
        "4. **Encoding and Standardization**:\n",
        "   - Encodes the `Severity` column into integer classes.\n",
        "   - Extracts and standardizes features for better neural network performance.\n",
        "\n",
        "5. **Data Splitting**:\n",
        "   - Splits the data into training and testing sets (80-20 split).\n",
        "\n",
        "The function returns the training and testing sets for both features and targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XtoOFcQmeESr"
      },
      "outputs": [],
      "source": [
        "def get_data(path):\n",
        "    try:\n",
        "        # Load dataset\n",
        "        df = pd.read_csv(path)\n",
        "    except FileNotFoundError:\n",
        "        return \"File not found\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while loading the dataset: {e}\"\n",
        "\n",
        "    # Convert time columns to datetime and compute duration (in minutes)\n",
        "    def parse_datetime(dt_str):\n",
        "      # Try formats with microseconds first, then without\n",
        "      for fmt in (\"%Y-%m-%d %H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S\"):\n",
        "          try:\n",
        "              return pd.to_datetime(dt_str, format=fmt)\n",
        "          except ValueError:\n",
        "              continue\n",
        "      # Return NaT if none of the formats work\n",
        "      return pd.NaT\n",
        "\n",
        "    # Apply the custom function to the relevant columns\n",
        "    df['Start_Time'] = df['Start_Time'].apply(parse_datetime)\n",
        "    df['End_Time'] = df['End_Time'].apply(parse_datetime)\n",
        "\n",
        "    # Drop rows where the datetime conversion failed\n",
        "    df = df.dropna(subset=['Start_Time', 'End_Time'])\n",
        "\n",
        "    df['Duration'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60.0\n",
        "\n",
        "    # Define the feature columns\n",
        "    features = [\n",
        "        'Distance(mi)',\n",
        "        'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng',\n",
        "        'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)',\n",
        "        'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)',\n",
        "        'Precipitation(in)',\n",
        "        'Duration'\n",
        "    ]\n",
        "\n",
        "    # Drop rows that have missing values in any of the selected features or in the target\n",
        "    df = df[['Severity'] + features].dropna()\n",
        "\n",
        "    # Encode Severity into integer classes (0, 1, 2, 3)\n",
        "    # This step will automatically map the unique severity values (e.g., 1, 2, 3, 4) to 0,1,2,3.\n",
        "    le = LabelEncoder()\n",
        "    df['Severity_encoded'] = le.fit_transform(df['Severity'])\n",
        "\n",
        "    # Prepare input features and target\n",
        "    X = df[features].values.astype(np.float32)\n",
        "    y = df['Severity_encoded'].values.astype(np.int32)\n",
        "\n",
        "    # Standardize features for better NN performance\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `train_nn` function builds, compiles, and trains a neural network model.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Building the Model**:\n",
        "   - Uses the `Sequential` API to define the neural network architecture.\n",
        "   - Adds three layers:\n",
        "     - A dense layer with 32 units and ReLU activation.\n",
        "     - A dense layer with 16 units and ReLU activation.\n",
        "     - A dense output layer with 4 units and softmax activation.\n",
        "\n",
        "2. **Compiling the Model**:\n",
        "   - Compiles the model with:\n",
        "     - `sparse_categorical_crossentropy` loss function.\n",
        "     - `adam` optimizer.\n",
        "     - `accuracy` as a metric.\n",
        "\n",
        "3. **Training the Model**:\n",
        "   - Trains the model using the training data (`X_train`, `y_train`).\n",
        "   - Specifies:\n",
        "     - 50 epochs.\n",
        "     - Batch size of 8192.\n",
        "     - 10% of the data for validation.\n",
        "     - Verbose output during training.\n",
        "\n",
        "The function returns the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b-oq-GQ5eESr"
      },
      "outputs": [],
      "source": [
        "def train_nn(X_train, y_train):\n",
        "    # Build the neural network model\n",
        "    model = Sequential([\n",
        "        Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=8192, validation_split=0.1, verbose=1)\n",
        "\n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code snippet loads and prepares the dataset for training and testing.\n",
        "\n",
        "This prepares the dataset for subsequent model training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv0AnPsOeESr"
      },
      "outputs": [],
      "source": [
        "csv = os.path.join(path, 'US_Constructions_Dec21.csv')\n",
        "X_train, X_test, y_train, y_test = get_data(csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\abhin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7292 - loss: 0.7515 - val_accuracy: 0.8870 - val_loss: 0.3967\n",
            "Epoch 2/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8873 - loss: 0.3929 - val_accuracy: 0.8870 - val_loss: 0.3861\n",
            "Epoch 3/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8869 - loss: 0.3847 - val_accuracy: 0.8870 - val_loss: 0.3791\n",
            "Epoch 4/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8868 - loss: 0.3778 - val_accuracy: 0.8869 - val_loss: 0.3712\n",
            "Epoch 5/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8868 - loss: 0.3685 - val_accuracy: 0.8874 - val_loss: 0.3589\n",
            "Epoch 6/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8878 - loss: 0.3553 - val_accuracy: 0.8911 - val_loss: 0.3468\n",
            "Epoch 7/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8914 - loss: 0.3453 - val_accuracy: 0.8928 - val_loss: 0.3424\n",
            "Epoch 8/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8933 - loss: 0.3412 - val_accuracy: 0.8938 - val_loss: 0.3396\n",
            "Epoch 9/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8941 - loss: 0.3381 - val_accuracy: 0.8943 - val_loss: 0.3376\n",
            "Epoch 10/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8942 - loss: 0.3375 - val_accuracy: 0.8946 - val_loss: 0.3360\n",
            "Epoch 11/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8949 - loss: 0.3350 - val_accuracy: 0.8950 - val_loss: 0.3343\n",
            "Epoch 12/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8954 - loss: 0.3328 - val_accuracy: 0.8949 - val_loss: 0.3335\n",
            "Epoch 13/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8955 - loss: 0.3318 - val_accuracy: 0.8951 - val_loss: 0.3324\n",
            "Epoch 14/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8957 - loss: 0.3301 - val_accuracy: 0.8952 - val_loss: 0.3305\n",
            "Epoch 15/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8958 - loss: 0.3288 - val_accuracy: 0.8951 - val_loss: 0.3289\n",
            "Epoch 16/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8952 - loss: 0.3279 - val_accuracy: 0.8951 - val_loss: 0.3273\n",
            "Epoch 17/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8955 - loss: 0.3259 - val_accuracy: 0.8953 - val_loss: 0.3256\n",
            "Epoch 18/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8958 - loss: 0.3249 - val_accuracy: 0.8958 - val_loss: 0.3241\n",
            "Epoch 19/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8964 - loss: 0.3231 - val_accuracy: 0.8968 - val_loss: 0.3223\n",
            "Epoch 20/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8971 - loss: 0.3208 - val_accuracy: 0.8969 - val_loss: 0.3208\n",
            "Epoch 21/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8975 - loss: 0.3194 - val_accuracy: 0.8974 - val_loss: 0.3194\n",
            "Epoch 22/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8978 - loss: 0.3185 - val_accuracy: 0.8978 - val_loss: 0.3180\n",
            "Epoch 23/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8981 - loss: 0.3170 - val_accuracy: 0.8976 - val_loss: 0.3166\n",
            "Epoch 24/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8983 - loss: 0.3162 - val_accuracy: 0.8984 - val_loss: 0.3158\n",
            "Epoch 25/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8989 - loss: 0.3143 - val_accuracy: 0.8986 - val_loss: 0.3146\n",
            "Epoch 26/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8989 - loss: 0.3140 - val_accuracy: 0.8987 - val_loss: 0.3142\n",
            "Epoch 27/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8995 - loss: 0.3127 - val_accuracy: 0.8988 - val_loss: 0.3132\n",
            "Epoch 28/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8995 - loss: 0.3126 - val_accuracy: 0.8993 - val_loss: 0.3127\n",
            "Epoch 29/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8992 - loss: 0.3128 - val_accuracy: 0.8991 - val_loss: 0.3123\n",
            "Epoch 30/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8996 - loss: 0.3119 - val_accuracy: 0.8993 - val_loss: 0.3120\n",
            "Epoch 31/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8998 - loss: 0.3110 - val_accuracy: 0.8993 - val_loss: 0.3114\n",
            "Epoch 32/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8998 - loss: 0.3111 - val_accuracy: 0.8993 - val_loss: 0.3114\n",
            "Epoch 33/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8999 - loss: 0.3110 - val_accuracy: 0.8999 - val_loss: 0.3116\n",
            "Epoch 34/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9000 - loss: 0.3107 - val_accuracy: 0.8997 - val_loss: 0.3107\n",
            "Epoch 35/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9000 - loss: 0.3104 - val_accuracy: 0.8996 - val_loss: 0.3103\n",
            "Epoch 36/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9003 - loss: 0.3094 - val_accuracy: 0.8999 - val_loss: 0.3100\n",
            "Epoch 37/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9004 - loss: 0.3097 - val_accuracy: 0.9000 - val_loss: 0.3097\n",
            "Epoch 38/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9003 - loss: 0.3097 - val_accuracy: 0.9002 - val_loss: 0.3098\n",
            "Epoch 39/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9003 - loss: 0.3097 - val_accuracy: 0.9000 - val_loss: 0.3100\n",
            "Epoch 40/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9005 - loss: 0.3091 - val_accuracy: 0.9002 - val_loss: 0.3092\n",
            "Epoch 41/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9005 - loss: 0.3088 - val_accuracy: 0.9003 - val_loss: 0.3093\n",
            "Epoch 42/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9003 - loss: 0.3092 - val_accuracy: 0.9003 - val_loss: 0.3091\n",
            "Epoch 43/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9006 - loss: 0.3086 - val_accuracy: 0.8998 - val_loss: 0.3092\n",
            "Epoch 44/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9004 - loss: 0.3087 - val_accuracy: 0.9007 - val_loss: 0.3087\n",
            "Epoch 45/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9005 - loss: 0.3090 - val_accuracy: 0.9002 - val_loss: 0.3086\n",
            "Epoch 46/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9008 - loss: 0.3080 - val_accuracy: 0.9005 - val_loss: 0.3081\n",
            "Epoch 47/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9007 - loss: 0.3082 - val_accuracy: 0.9002 - val_loss: 0.3082\n",
            "Epoch 48/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9009 - loss: 0.3079 - val_accuracy: 0.9007 - val_loss: 0.3082\n",
            "Epoch 49/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9009 - loss: 0.3077 - val_accuracy: 0.9007 - val_loss: 0.3078\n",
            "Epoch 50/50\n",
            "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9008 - loss: 0.3076 - val_accuracy: 0.9007 - val_loss: 0.3077\n"
          ]
        }
      ],
      "source": [
        "model = train_nn(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.3074662983417511\n",
            "Test accuracy: 0.9007605314254761\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
