\section{Conclusion}

In this paper, we presented a comprehensive study
on parameter reduction and quantization of image classification models using knowledge distillation.
We demonstrated that knowledge distillation is an effective technique
for transferring knowledge from a larger, more complex model (the teacher)
to a smaller, more efficient model (the student)
in the context of image classification tasks.
Based on these results, we believe that our knowledge distillation framework can be extended
to other computer vision tasks, including image inpainting,
without significant performance degradation.
We anticipate that this framework will contribute to the development of more efficient models
across a wide range of computer vision applications.
